{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742897476.615879  110872 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742897476.620791  110872 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742897476.634169  110872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742897476.634183  110872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742897476.634184  110872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742897476.634185  110872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    "os.environ['GLOB_v'] = '0'\n",
    "os.environ[\"GLOG_minloglevel\"] = \"3\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.cuda import empty_cache\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "\n",
    "size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x, y 좌표를 이용한 운동 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 14:01:53.353745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743483713.370481  131408 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743483713.375727  131408 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743483713.470616  131408 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743483713.470637  131408 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743483713.470639  131408 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743483713.470640  131408 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 14:01:53.491128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743483717.196513  131408 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743483717.199264  131489 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743483717.273124  131473 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743483717.305313  131480 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743483717.559997  131408 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4225 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "W0000 00:00:1743483719.653093  131475 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    126\u001b[39m     cv2.putText(frame, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, (\u001b[32m10\u001b[39m, \u001b[32m45\u001b[39m), cv2.FONT_HERSHEY_SIMPLEX, \u001b[32m2\u001b[39m, (\u001b[32m255\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m), \u001b[32m3\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mExercise Classifier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv2.waitKey(\u001b[32m1\u001b[39m) & \u001b[32m0xFF\u001b[39m == \u001b[38;5;28mord\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    134\u001b[39m cap.release()\n\u001b[32m    135\u001b[39m mp_pose.close()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdestroyAllWindows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque, Counter\n",
    "from keras.models import load_model\n",
    "import threading\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "from gtts import gTTS\n",
    "\n",
    "def extract_pose_landmarks(results):\n",
    "    xyz_list = []\n",
    "\n",
    "    points = [\n",
    "        11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28\n",
    "    ]\n",
    "\n",
    "    lm = results.pose_landmarks.landmark\n",
    "\n",
    "    neck_x = (lm[11].x + lm[12].x) / 2\n",
    "    neck_y = (lm[11].y + lm[12].y) / 2\n",
    "    #neck_z = (lm[11].z + lm[12].z) / 2\n",
    "\n",
    "    for idx in points:\n",
    "        x = lm[idx].x - neck_x\n",
    "        y = lm[idx].y - neck_y\n",
    "        #z = lm[idx].z - neck_z\n",
    "\n",
    "        xyz_list.append([x, y])\n",
    "\n",
    "    return xyz_list\n",
    "\n",
    "# ---- TTS Thread ----\n",
    "class TextToSpeechThread(threading.Thread):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            tts = gTTS(text=self.text, lang='ko')\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmpfile:\n",
    "                tts.save(tmpfile.name)\n",
    "                os.system(f\"mpg123 {tmpfile.name}\")\n",
    "                os.remove(tmpfile.name)\n",
    "        except Exception as e:\n",
    "            print(f\"[TTS Error] {e}\")\n",
    "\n",
    "# ---- Prediction Thread ----\n",
    "class PredictionThread(threading.Thread):\n",
    "    def __init__(self, model, sequence, lock):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sequence = sequence\n",
    "        self.lock = lock\n",
    "        self.result = None\n",
    "        self.running = True\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            with self.lock:\n",
    "                if len(self.sequence) == 20:\n",
    "                    input_data = np.array(list(self.sequence)).reshape(1, 20, 24)\n",
    "                else:\n",
    "                    input_data = None\n",
    "\n",
    "            if input_data is not None:\n",
    "                prediction = self.model.predict(input_data, verbose=0)\n",
    "                self.result = prediction\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "sequence = deque(maxlen=20)\n",
    "\n",
    "execrise_list = [\"Standing\", \"Standing Knee Raise\", \"Shoulder Press\", \"Squat\", \"Side Lunge\"]\n",
    "model = load_model('exercise_classifier.h5')\n",
    "\n",
    "lock = threading.Lock()\n",
    "predict_thread = PredictionThread(model, sequence, lock)\n",
    "predict_thread.start()\n",
    "\n",
    "last_exercise = None\n",
    "last_tts_time = 0  # TTS 중복 방지 타이머\n",
    "\n",
    "try:\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            #image = cv2.resize(image, (640, 480))\n",
    "\n",
    "            results = mp_pose.process(image)\n",
    "            if results.pose_landmarks is None:\n",
    "                continue\n",
    "\n",
    "            landmarks = extract_pose_landmarks(results)\n",
    "\n",
    "            with lock:\n",
    "                sequence.append(landmarks)\n",
    "\n",
    "            if predict_thread.result is not None:\n",
    "                predict = predict_thread.result\n",
    "                predict_class = int(np.argmax(predict))\n",
    "                label = execrise_list[predict_class]\n",
    "\n",
    "                # 이전에 예측된 운동과 현재 운동이 다를 경우 다시 예측된 운동명 TTS 재생\n",
    "                if label != last_exercise:\n",
    "                    if time.time() - last_tts_time > 5:\n",
    "                        tts_thread = TextToSpeechThread(f\"{label}\")\n",
    "                        tts_thread.start()\n",
    "                        last_exercise = label\n",
    "                        last_tts_time = time.time()\n",
    "\n",
    "                # 화면에 예측 표시\n",
    "                cv2.putText(frame, f\"{label}\", (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "\n",
    "            cv2.imshow('Exercise Classifier', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "finally:\n",
    "    predict_thread.stop()\n",
    "    predict_thread.join()\n",
    "    cap.release()\n",
    "    mp_pose.close()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x, y좌표 + 관절 각도를 이용한 운동 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:41:18.463071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743057678.479619    4383 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743057678.485218    4383 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743057678.499100    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057678.499115    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057678.499117    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057678.499118    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 15:41:18.503961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743057680.927604    4383 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743057680.929935    6164 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743057681.023162    6150 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057681.060775    6153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743057681.079283    4383 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1176 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "W0000 00:00:1743057683.000224    6155 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "I0000 00:00:1743057685.415758    6193 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpbasly5fq.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpbasly5fq.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpitpypeuq.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpitpypeuq.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp2fu8wj3s.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmp2fu8wj3s.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpqyu0edv0.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpqyu0edv0.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp4i9ep5z4.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmp4i9ep5z4.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpnmt552aw.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpnmt552aw.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpthya3weh.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpthya3weh.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpb4p98am_.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpb4p98am_.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpivxm4q20.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpivxm4q20.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp8l8a7edw.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmp8l8a7edw.mp3 finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import mediapipe as mp\n",
    "from collections import deque, Counter\n",
    "import threading\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "from gtts import gTTS\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def extract_pose_angles(results, frame):\n",
    "    lm = results.pose_landmarks.landmark\n",
    "    image_height, image_width, _ = frame.shape\n",
    "\n",
    "    neck_x = (lm[11].x + lm[12].x) / 2\n",
    "    neck_y = (lm[11].y + lm[12].y) / 2\n",
    "    landmarks = [(lm[i].x - neck_x, lm[i].y - neck_y) for i in range(33)]\n",
    "\n",
    "    angle_joints = [\n",
    "        (11, 13, 15), (12, 14, 16),  # 양팔\n",
    "        (23, 25, 27), (24, 26, 28),  # 양다리\n",
    "        (13, 11, 23), (14, 12, 24),  # 상체\n",
    "        (11, 23, 25), (12, 24, 26),  # 골반\n",
    "    ]\n",
    "\n",
    "    landmark_points = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "    xy_features = []\n",
    "    for i in landmark_points:\n",
    "        x, y = landmarks[i]\n",
    "        xy_features.extend([x, y])\n",
    "\n",
    "    angles = []\n",
    "    features_per_video = []\n",
    "    for a, b, c in angle_joints:\n",
    "        angle = calculate_angle(landmarks[a], landmarks[b], landmarks[c])\n",
    "        angles.append(angle)\n",
    "\n",
    "        cx = int(lm[b].x * image_width)\n",
    "        cy = int(lm[b].y * image_height)\n",
    "        cv2.putText(frame, f\"{int(angle)}\", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    full_feature = xy_features + angles\n",
    "    features_per_video.append(full_feature)\n",
    "    \n",
    "    return features_per_video\n",
    "\n",
    "class TextToSpeechThread(threading.Thread):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            tts = gTTS(text=self.text, lang='ko')\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmpfile:\n",
    "                tts.save(tmpfile.name)\n",
    "                os.system(f\"mpg123 {tmpfile.name}\")\n",
    "                os.remove(tmpfile.name)\n",
    "        except Exception as e:\n",
    "            print(f\"[TTS Error] {e}\")\n",
    "\n",
    "class PredictionThread(threading.Thread):\n",
    "    def __init__(self, model, sequence, lock):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sequence = sequence\n",
    "        self.lock = lock\n",
    "        self.result = None\n",
    "        self.running = True\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            with self.lock:\n",
    "                if len(self.sequence) == 20:\n",
    "                    input_data = np.array(list(self.sequence)).reshape(1, 20, 32)\n",
    "                else:\n",
    "                    input_data = None\n",
    "\n",
    "            if input_data is not None:\n",
    "                prediction = self.model.predict(input_data, verbose=0)\n",
    "                self.result = prediction\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "sequence = deque(maxlen=20)\n",
    "\n",
    "execrise_list = [\"Standing\", \"Standing Knee Raise\", \"Shoulder Press\", \"Squat\", \"Front Lunge\"]\n",
    "model = load_model(\"exercise_classifier_angle.h5\")\n",
    "\n",
    "lock = threading.Lock()\n",
    "predict_thread = PredictionThread(model, sequence, lock)\n",
    "predict_thread.start()\n",
    "\n",
    "fix_exercise = False\n",
    "fixed_class = None\n",
    "last_tts_time = 0\n",
    "last_exercise = None\n",
    "\n",
    "try:\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # frame = cv2.resize(frame, (1280, 720))\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = mp_pose.process(image)\n",
    "            if results.pose_landmarks is None:\n",
    "                continue\n",
    "\n",
    "            features = extract_pose_angles(results, image)\n",
    "            with lock:\n",
    "                sequence.append(features)\n",
    "\n",
    "            if predict_thread.result is not None:\n",
    "                predict = predict_thread.result\n",
    "                predict_class = int(np.argmax(predict))\n",
    "                label = execrise_list[predict_class]\n",
    "\n",
    "                # 이전에 예측된 운동과 현재 운동이 다를 경우 다시 예측된 운동명 TTS 재생\n",
    "                if label != last_exercise:\n",
    "                    if time.time() - last_tts_time > 5:\n",
    "                        tts_thread = TextToSpeechThread(f\"{label}\")\n",
    "                        tts_thread.start()\n",
    "                        last_tts_time = time.time()\n",
    "                        last_exercise = label\n",
    "\n",
    "                cv2.putText(frame, f\"{label}\", (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "\n",
    "            cv2.imshow('Exercise Classifier', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "finally:\n",
    "    predict_thread.stop()\n",
    "    predict_thread.join()\n",
    "    cap.release()\n",
    "    mp_pose.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
