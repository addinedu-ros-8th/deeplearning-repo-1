{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def extract_landmarks(image, pose):\n",
    "    \"\"\"MediaPipe Pose를 사용하여 33개 관절 좌표를 추출\"\"\"\n",
    "    results = pose.process(image)\n",
    "    if not results.pose_landmarks:\n",
    "        return None\n",
    "    landmarks = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        landmarks.extend([lm.x, lm.y, lm.z])\n",
    "    return landmarks\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"세 점 (a, b, c)으로 이루어진 각도를 계산\"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    \n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    return np.degrees(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(dataset_path, sequence_length=30):\n",
    "    \"\"\"dataset 폴더 내 모든 운동 폴더를 불러와 데이터를 생성하고 라벨링\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "    \n",
    "    label_map = {folder: i for i, folder in enumerate(os.listdir(dataset_path))}  # 폴더명을 라벨로 매핑\n",
    "    print(f\"📌 라벨 매핑: {label_map}\")  # { 'pushup': 0, 'shoulder_press': 1, ... }\n",
    "    \n",
    "    for folder, label in label_map.items():\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if not os.path.isdir(folder_path):  \n",
    "            continue  # 폴더가 아닌 파일이 있으면 무시\n",
    "        \n",
    "        print(f\"🔍 {folder} 영상 처리 중... (라벨 {label})\")\n",
    "        \n",
    "        for video_name in os.listdir(folder_path):\n",
    "            video_path = os.path.join(folder_path, video_name)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            sequence = []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                landmarks = extract_landmarks(frame_rgb, pose)\n",
    "\n",
    "                if landmarks:\n",
    "                    sequence.append(landmarks)\n",
    "                    if len(sequence) == sequence_length:\n",
    "                        data.append(sequence[:])\n",
    "                        labels.append(label)\n",
    "                        sequence.pop(0)  # 슬라이딩 윈도우 적용 (가장 오래된 프레임 삭제)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "    pose.close()\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742454970.794570   98930 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742454970.796996   99235 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742454970.857296   99221 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742454970.886060   99219 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742454970.905199   99230 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 라벨 매핑: {'standing': 0, 'pushup': 1, 'stand_knee_raise': 2, 'stand_shoulder_press': 3}\n",
      "🔍 standing 영상 처리 중... (라벨 0)\n",
      "🔍 pushup 영상 처리 중... (라벨 1)\n",
      "🔍 stand_knee_raise 영상 처리 중... (라벨 2)\n",
      "🔍 stand_shoulder_press 영상 처리 중... (라벨 3)\n",
      "✅ 데이터셋 크기: (10769, 30, 99), 라벨 크기: (10769, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/shin/deeplearning-repo-1/dataset\"\n",
    "X, y = process_videos(dataset_path)\n",
    "\n",
    "# 라벨을 원-핫 인코딩\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(set(y)))\n",
    "\n",
    "# 데이터셋 분할 (학습: 80%, 테스트: 20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"✅ 데이터셋 크기: {X.shape}, 라벨 크기: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/venv/project/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 구성\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(30, 99)),  # 33개 관절 * 3(x,y,z)\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3개의 운동 분류\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습 (X_train, y_train 준비 필요)\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8615, 30, 99), (8615, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 17:27:14.065731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742459234.156970    4757 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742459234.184317    4757 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742459234.390509    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742459234.390531    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742459234.390532    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742459234.390533    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-20 17:27:14.414824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742459248.771324    4757 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742459248.774150    5509 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742459248.892202    5503 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742459248.938615    5504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742459248.968797    5503 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "DATASET_DIR = '/home/shin/deeplearning-repo-1/dataset'\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for label_folder in os.listdir(DATASET_DIR):\n",
    "    label_path = os.path.join(DATASET_DIR, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "    for video_file in os.listdir(label_path):\n",
    "        video_path = os.path.join(label_path, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = []\n",
    "                for lm in results.pose_landmarks.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "                frames.append(landmarks)\n",
    "                if len(frames) >= SEQUENCE_LENGTH:\n",
    "                    sequences.append(frames[-SEQUENCE_LENGTH:])\n",
    "                    labels.append(label_folder)\n",
    "        cap.release()\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10769, 30, 132), (10769, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(labels_categorical)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742460053.278416    4757 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4227 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/shin/venv/project/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',      # validation loss를 모니터링\n",
    "    patience=10,         # 10 epoch 동안 개선 없으면 멈춤\n",
    "    restore_best_weights=True # 가장 좋은 가중치 복원\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, len(landmarks))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742460057.734958    7014 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7693 - loss: 0.6215 - val_accuracy: 0.9865 - val_loss: 0.0719\n",
      "Epoch 2/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9923 - loss: 0.0447 - val_accuracy: 1.0000 - val_loss: 5.6773e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 1.3082e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 5.2862e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.1940e-04 - val_accuracy: 1.0000 - val_loss: 2.6629e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.0172e-04 - val_accuracy: 1.0000 - val_loss: 1.4121e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.7631e-04 - val_accuracy: 1.0000 - val_loss: 9.0337e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.7999e-04 - val_accuracy: 1.0000 - val_loss: 5.7821e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.1717e-04 - val_accuracy: 1.0000 - val_loss: 3.9460e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.9678e-04 - val_accuracy: 1.0000 - val_loss: 2.7300e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1204e-04 - val_accuracy: 1.0000 - val_loss: 2.0224e-06\n",
      "Epoch 12/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6880e-04 - val_accuracy: 1.0000 - val_loss: 1.4239e-06\n",
      "Epoch 13/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4920e-04 - val_accuracy: 1.0000 - val_loss: 1.0353e-06\n",
      "Epoch 14/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9536 - loss: 0.1821 - val_accuracy: 0.6300 - val_loss: 1.0452\n",
      "Epoch 15/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6070 - loss: 0.8730 - val_accuracy: 0.7136 - val_loss: 0.5571\n",
      "Epoch 16/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7506 - loss: 0.5177 - val_accuracy: 0.9526 - val_loss: 0.1373\n",
      "Epoch 17/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9465 - loss: 0.1667 - val_accuracy: 0.8863 - val_loss: 0.2417\n",
      "Epoch 18/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9770 - loss: 0.0791 - val_accuracy: 0.9981 - val_loss: 0.0096\n",
      "Epoch 19/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9969 - loss: 0.0191 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 20/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 3.0182e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.2714e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 7.2769e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.5401e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d324ecbf2c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=100, callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('exercise_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m vote_buffer = deque(maxlen=\u001b[32m30\u001b[39m) \n\u001b[32m      6\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/home/shin/deeplearning-repo-1/dataset/stand_shoulder_press/untitled.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m cap = \u001b[43mcv2\u001b[49m.VideoCapture(\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m sequence_buffer = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_angle\u001b[39m(a, b, c):\n",
      "\u001b[31mNameError\u001b[39m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import deque, Counter\n",
    "\n",
    "vote_buffer = deque(maxlen=30) \n",
    "\n",
    "path = \"/home/shin/deeplearning-repo-1/dataset/stand_shoulder_press/untitled.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_buffer = []\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    radians = np.arccos(np.clip(np.dot(a - b, c - b) / (np.linalg.norm(a - b) * np.linalg.norm(c - b)), -1.0, 1.0))\n",
    "    return np.degrees(radians)\n",
    "\n",
    "joint_angle_data = {}\n",
    "for seq, label in zip(sequences, labels):\n",
    "    if label not in joint_angle_data:\n",
    "        joint_angle_data[label] = {\"left_elbow\": []}\n",
    "    for frame in seq:\n",
    "        landmarks = np.array(frame).reshape(-1, 4)  # (33, 4)\n",
    "        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value][:2]\n",
    "        left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value][:2]\n",
    "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value][:2]\n",
    "        angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "        joint_angle_data[label][\"left_elbow\"].append(angle)\n",
    "\n",
    "target_pose = {}\n",
    "for label, angles in joint_angle_data.items():\n",
    "    target_pose[label] = {}\n",
    "    for joint, values in angles.items():\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        target_pose[label][joint] = (mean - 0.8 * std, mean + 0.8 * std)\n",
    "\n",
    "def draw_correction(frame, angle, expected_range, joint_pos):\n",
    "    min_angle, max_angle = expected_range\n",
    "    if angle < min_angle:\n",
    "        direction = \"UP\"\n",
    "    elif angle > max_angle:\n",
    "        direction = \"DOWN\"\n",
    "    else:\n",
    "        direction = None\n",
    "\n",
    "    cv2.circle(frame, joint_pos, 10, (0, 0, 255), -1)\n",
    "    if direction:\n",
    "        if direction == \"UP\":\n",
    "            cv2.arrowedLine(frame, joint_pos, (joint_pos[0], joint_pos[1]-50), (0, 0, 255), 3, tipLength=0.5)\n",
    "        elif direction == \"DOWN\":\n",
    "            cv2.arrowedLine(frame, joint_pos, (joint_pos[0], joint_pos[1]+50), (0, 0, 255), 3, tipLength=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_buffer = []\n",
    "vote_buffer = deque(maxlen=30)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "        sequence_buffer.append(landmarks)\n",
    "\n",
    "        if len(sequence_buffer) >= SEQUENCE_LENGTH:\n",
    "            input_seq = np.expand_dims(sequence_buffer[-SEQUENCE_LENGTH:], axis=0)\n",
    "            prediction = model.predict(input_seq)\n",
    "            predicted_label = le.inverse_transform([np.argmax(prediction)])[0]\n",
    "            vote_buffer.append(predicted_label)\n",
    "\n",
    "            if len(vote_buffer) == vote_buffer.maxlen:\n",
    "                most_common_label = Counter(vote_buffer).most_common(1)[0][0]\n",
    "                cv2.putText(frame, f'Mode: {most_common_label}', (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # 교정 가이드라인 (자동화된 기준값 사용)\n",
    "                lm = results.pose_landmarks.landmark\n",
    "                left_shoulder = (int(lm[mp_pose.PoseLandmark.LEFT_SHOULDER].x * w),\n",
    "                                 int(lm[mp_pose.PoseLandmark.LEFT_SHOULDER].y * h))\n",
    "                left_elbow = (int(lm[mp_pose.PoseLandmark.LEFT_ELBOW].x * w),\n",
    "                              int(lm[mp_pose.PoseLandmark.LEFT_ELBOW].y * h))\n",
    "                left_wrist = (int(lm[mp_pose.PoseLandmark.LEFT_WRIST].x * w),\n",
    "                              int(lm[mp_pose.PoseLandmark.LEFT_WRIST].y * h))\n",
    "\n",
    "                angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "                expected_range = target_pose[most_common_label][\"left_elbow\"]\n",
    "                draw_correction(frame, angle, expected_range, left_elbow)\n",
    "\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
