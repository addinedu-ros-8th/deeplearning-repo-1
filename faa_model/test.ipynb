{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def extract_landmarks(image, pose):\n",
    "    \"\"\"MediaPipe Poseë¥¼ ì‚¬ìš©í•˜ì—¬ 33ê°œ ê´€ì ˆ ì¢Œí‘œë¥¼ ì¶”ì¶œ\"\"\"\n",
    "    results = pose.process(image)\n",
    "    if not results.pose_landmarks:\n",
    "        return None\n",
    "    landmarks = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        landmarks.extend([lm.x, lm.y, lm.z])\n",
    "    return landmarks\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"ì„¸ ì  (a, b, c)ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê°ë„ë¥¼ ê³„ì‚°\"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    \n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    return np.degrees(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(dataset_path, sequence_length=30):\n",
    "    \"\"\"dataset í´ë” ë‚´ ëª¨ë“  ìš´ë™ í´ë”ë¥¼ ë¶ˆëŸ¬ì™€ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ë¼ë²¨ë§\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "    \n",
    "    label_map = {folder: i for i, folder in enumerate(os.listdir(dataset_path))}  # í´ë”ëª…ì„ ë¼ë²¨ë¡œ ë§¤í•‘\n",
    "    print(f\"ğŸ“Œ ë¼ë²¨ ë§¤í•‘: {label_map}\")  # { 'pushup': 0, 'shoulder_press': 1, ... }\n",
    "    \n",
    "    for folder, label in label_map.items():\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if not os.path.isdir(folder_path):  \n",
    "            continue  # í´ë”ê°€ ì•„ë‹Œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¬´ì‹œ\n",
    "        \n",
    "        print(f\"ğŸ” {folder} ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ë¼ë²¨ {label})\")\n",
    "        \n",
    "        for video_name in os.listdir(folder_path):\n",
    "            video_path = os.path.join(folder_path, video_name)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            sequence = []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                landmarks = extract_landmarks(frame_rgb, pose)\n",
    "\n",
    "                if landmarks:\n",
    "                    sequence.append(landmarks)\n",
    "                    if len(sequence) == sequence_length:\n",
    "                        data.append(sequence[:])\n",
    "                        labels.append(label)\n",
    "                        sequence.pop(0)  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš© (ê°€ì¥ ì˜¤ë˜ëœ í”„ë ˆì„ ì‚­ì œ)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "    pose.close()\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742454970.794570   98930 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742454970.796996   99235 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742454970.857296   99221 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742454970.886060   99219 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742454970.905199   99230 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ë¼ë²¨ ë§¤í•‘: {'standing': 0, 'pushup': 1, 'stand_knee_raise': 2, 'stand_shoulder_press': 3}\n",
      "ğŸ” standing ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ë¼ë²¨ 0)\n",
      "ğŸ” pushup ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ë¼ë²¨ 1)\n",
      "ğŸ” stand_knee_raise ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ë¼ë²¨ 2)\n",
      "ğŸ” stand_shoulder_press ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ë¼ë²¨ 3)\n",
      "âœ… ë°ì´í„°ì…‹ í¬ê¸°: (10769, 30, 99), ë¼ë²¨ í¬ê¸°: (10769, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/shin/deeplearning-repo-1/dataset\"\n",
    "X, y = process_videos(dataset_path)\n",
    "\n",
    "# ë¼ë²¨ì„ ì›-í•« ì¸ì½”ë”©\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(set(y)))\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶„í•  (í•™ìŠµ: 80%, í…ŒìŠ¤íŠ¸: 20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ í¬ê¸°: {X.shape}, ë¼ë²¨ í¬ê¸°: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/venv/project/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# LSTM ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(30, 99)),  # 33ê°œ ê´€ì ˆ * 3(x,y,z)\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3ê°œì˜ ìš´ë™ ë¶„ë¥˜\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ (X_train, y_train ì¤€ë¹„ í•„ìš”)\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8615, 30, 99), (8615, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 17:27:14.065731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742459234.156970    4757 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742459234.184317    4757 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742459234.390509    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742459234.390531    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742459234.390532    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742459234.390533    4757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-20 17:27:14.414824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742459248.771324    4757 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742459248.774150    5509 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742459248.892202    5503 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742459248.938615    5504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742459248.968797    5503 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "DATASET_DIR = '/home/shin/deeplearning-repo-1/dataset'\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for label_folder in os.listdir(DATASET_DIR):\n",
    "    label_path = os.path.join(DATASET_DIR, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "    for video_file in os.listdir(label_path):\n",
    "        video_path = os.path.join(label_path, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = []\n",
    "                for lm in results.pose_landmarks.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "                frames.append(landmarks)\n",
    "                if len(frames) >= SEQUENCE_LENGTH:\n",
    "                    sequences.append(frames[-SEQUENCE_LENGTH:])\n",
    "                    labels.append(label_folder)\n",
    "        cap.release()\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10769, 30, 132), (10769, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(labels_categorical)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742460053.278416    4757 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4227 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/shin/venv/project/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',      # validation lossë¥¼ ëª¨ë‹ˆí„°ë§\n",
    "    patience=10,         # 10 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ë©ˆì¶¤\n",
    "    restore_best_weights=True # ê°€ì¥ ì¢‹ì€ ê°€ì¤‘ì¹˜ ë³µì›\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, len(landmarks))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742460057.734958    7014 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7693 - loss: 0.6215 - val_accuracy: 0.9865 - val_loss: 0.0719\n",
      "Epoch 2/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9923 - loss: 0.0447 - val_accuracy: 1.0000 - val_loss: 5.6773e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 1.3082e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 5.2862e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.1940e-04 - val_accuracy: 1.0000 - val_loss: 2.6629e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.0172e-04 - val_accuracy: 1.0000 - val_loss: 1.4121e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.7631e-04 - val_accuracy: 1.0000 - val_loss: 9.0337e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.7999e-04 - val_accuracy: 1.0000 - val_loss: 5.7821e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.1717e-04 - val_accuracy: 1.0000 - val_loss: 3.9460e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.9678e-04 - val_accuracy: 1.0000 - val_loss: 2.7300e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1204e-04 - val_accuracy: 1.0000 - val_loss: 2.0224e-06\n",
      "Epoch 12/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6880e-04 - val_accuracy: 1.0000 - val_loss: 1.4239e-06\n",
      "Epoch 13/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4920e-04 - val_accuracy: 1.0000 - val_loss: 1.0353e-06\n",
      "Epoch 14/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9536 - loss: 0.1821 - val_accuracy: 0.6300 - val_loss: 1.0452\n",
      "Epoch 15/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6070 - loss: 0.8730 - val_accuracy: 0.7136 - val_loss: 0.5571\n",
      "Epoch 16/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7506 - loss: 0.5177 - val_accuracy: 0.9526 - val_loss: 0.1373\n",
      "Epoch 17/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9465 - loss: 0.1667 - val_accuracy: 0.8863 - val_loss: 0.2417\n",
      "Epoch 18/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9770 - loss: 0.0791 - val_accuracy: 0.9981 - val_loss: 0.0096\n",
      "Epoch 19/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9969 - loss: 0.0191 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 20/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 3.0182e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.2714e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 7.2769e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.5401e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d324ecbf2c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=100, callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('exercise_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m vote_buffer = deque(maxlen=\u001b[32m30\u001b[39m) \n\u001b[32m      6\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/home/shin/deeplearning-repo-1/dataset/stand_shoulder_press/untitled.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m cap = \u001b[43mcv2\u001b[49m.VideoCapture(\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m sequence_buffer = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_angle\u001b[39m(a, b, c):\n",
      "\u001b[31mNameError\u001b[39m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import deque, Counter\n",
    "\n",
    "vote_buffer = deque(maxlen=30) \n",
    "\n",
    "path = \"/home/shin/deeplearning-repo-1/dataset/stand_shoulder_press/untitled.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_buffer = []\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    radians = np.arccos(np.clip(np.dot(a - b, c - b) / (np.linalg.norm(a - b) * np.linalg.norm(c - b)), -1.0, 1.0))\n",
    "    return np.degrees(radians)\n",
    "\n",
    "joint_angle_data = {}\n",
    "for seq, label in zip(sequences, labels):\n",
    "    if label not in joint_angle_data:\n",
    "        joint_angle_data[label] = {\"left_elbow\": []}\n",
    "    for frame in seq:\n",
    "        landmarks = np.array(frame).reshape(-1, 4)  # (33, 4)\n",
    "        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value][:2]\n",
    "        left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value][:2]\n",
    "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value][:2]\n",
    "        angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "        joint_angle_data[label][\"left_elbow\"].append(angle)\n",
    "\n",
    "target_pose = {}\n",
    "for label, angles in joint_angle_data.items():\n",
    "    target_pose[label] = {}\n",
    "    for joint, values in angles.items():\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        target_pose[label][joint] = (mean - 0.8 * std, mean + 0.8 * std)\n",
    "\n",
    "def draw_correction(frame, angle, expected_range, joint_pos):\n",
    "    min_angle, max_angle = expected_range\n",
    "    if angle < min_angle:\n",
    "        direction = \"UP\"\n",
    "    elif angle > max_angle:\n",
    "        direction = \"DOWN\"\n",
    "    else:\n",
    "        direction = None\n",
    "\n",
    "    cv2.circle(frame, joint_pos, 10, (0, 0, 255), -1)\n",
    "    if direction:\n",
    "        if direction == \"UP\":\n",
    "            cv2.arrowedLine(frame, joint_pos, (joint_pos[0], joint_pos[1]-50), (0, 0, 255), 3, tipLength=0.5)\n",
    "        elif direction == \"DOWN\":\n",
    "            cv2.arrowedLine(frame, joint_pos, (joint_pos[0], joint_pos[1]+50), (0, 0, 255), 3, tipLength=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_buffer = []\n",
    "vote_buffer = deque(maxlen=30)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "        sequence_buffer.append(landmarks)\n",
    "\n",
    "        if len(sequence_buffer) >= SEQUENCE_LENGTH:\n",
    "            input_seq = np.expand_dims(sequence_buffer[-SEQUENCE_LENGTH:], axis=0)\n",
    "            prediction = model.predict(input_seq)\n",
    "            predicted_label = le.inverse_transform([np.argmax(prediction)])[0]\n",
    "            vote_buffer.append(predicted_label)\n",
    "\n",
    "            if len(vote_buffer) == vote_buffer.maxlen:\n",
    "                most_common_label = Counter(vote_buffer).most_common(1)[0][0]\n",
    "                cv2.putText(frame, f'Mode: {most_common_label}', (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # êµì • ê°€ì´ë“œë¼ì¸ (ìë™í™”ëœ ê¸°ì¤€ê°’ ì‚¬ìš©)\n",
    "                lm = results.pose_landmarks.landmark\n",
    "                left_shoulder = (int(lm[mp_pose.PoseLandmark.LEFT_SHOULDER].x * w),\n",
    "                                 int(lm[mp_pose.PoseLandmark.LEFT_SHOULDER].y * h))\n",
    "                left_elbow = (int(lm[mp_pose.PoseLandmark.LEFT_ELBOW].x * w),\n",
    "                              int(lm[mp_pose.PoseLandmark.LEFT_ELBOW].y * h))\n",
    "                left_wrist = (int(lm[mp_pose.PoseLandmark.LEFT_WRIST].x * w),\n",
    "                              int(lm[mp_pose.PoseLandmark.LEFT_WRIST].y * h))\n",
    "\n",
    "                angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "                expected_range = target_pose[most_common_label][\"left_elbow\"]\n",
    "                draw_correction(frame, angle, expected_range, left_elbow)\n",
    "\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
