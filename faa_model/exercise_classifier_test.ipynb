{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x, y 좌표를 이용한 운동 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743559938.006961  634561 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743559938.008308  791145 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743559938.078420  791131 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "W0000 00:00:1743559938.125898  791132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpgi74y0ft.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:00] Decoding of tmpgi74y0ft.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpb8d2sruk.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:00] Decoding of tmpb8d2sruk.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpgzu1i2zl.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:00] Decoding of tmpgzu1i2zl.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmppolj53zk.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:00] Decoding of tmppolj53zk.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp9m0vikr4.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:00] Decoding of tmp9m0vikr4.mp3 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp0vol8801.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:00] Decoding of tmp0vol8801.mp3 finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque, Counter\n",
    "from keras.models import load_model\n",
    "import threading\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "from gtts import gTTS\n",
    "\n",
    "def extract_pose_landmarks(results):\n",
    "    xyz_list = []\n",
    "\n",
    "    points = [\n",
    "        11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28\n",
    "    ]\n",
    "\n",
    "    lm = results.pose_landmarks.landmark\n",
    "\n",
    "    neck_x = (lm[11].x + lm[12].x) / 2\n",
    "    neck_y = (lm[11].y + lm[12].y) / 2\n",
    "    #neck_z = (lm[11].z + lm[12].z) / 2\n",
    "\n",
    "    for idx in points:\n",
    "        x = lm[idx].x - neck_x\n",
    "        y = lm[idx].y - neck_y\n",
    "        #z = lm[idx].z - neck_z\n",
    "\n",
    "        xyz_list.append([x, y])\n",
    "\n",
    "    return xyz_list\n",
    "\n",
    "# ---- TTS Thread ----\n",
    "class TextToSpeechThread(threading.Thread):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            tts = gTTS(text=self.text, lang='ko')\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmpfile:\n",
    "                tts.save(tmpfile.name)\n",
    "                os.system(f\"mpg123 {tmpfile.name}\")\n",
    "                os.remove(tmpfile.name)\n",
    "        except Exception as e:\n",
    "            print(f\"[TTS Error] {e}\")\n",
    "\n",
    "# ---- Prediction Thread ----\n",
    "class PredictionThread(threading.Thread):\n",
    "    def __init__(self, model, sequence, lock):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sequence = sequence\n",
    "        self.lock = lock\n",
    "        self.result = None\n",
    "        self.running = True\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            with self.lock:\n",
    "                if len(self.sequence) == 20:\n",
    "                    input_data = np.array(list(self.sequence)).reshape(1, 20, 24)\n",
    "                else:\n",
    "                    input_data = None\n",
    "\n",
    "            if input_data is not None:\n",
    "                prediction = self.model.predict(input_data, verbose=0)\n",
    "                self.result = prediction\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "\n",
    "class AngleGuid():\n",
    "    def __init__(self, exercise):\n",
    "        self.vectors = {0: None, 1: None}\n",
    "        self.initialized = {0: False, 1: False}\n",
    "        self.count = 0\n",
    "\n",
    "        self.r = (0, 0, 255)\n",
    "        self.g = (0, 255, 0)\n",
    "        self.b = (255, 0, 0)\n",
    "        self.p = (147, 20, 255)\n",
    "\n",
    "        self.state = {0: \"up\", 1: \"down\"}\n",
    "        self.exercise = exercise\n",
    "\n",
    "        self.last_tts_time = 0\n",
    "        self.last_time = 0\n",
    "        self.tts_play = False\n",
    "\n",
    "        if self.exercise == \"squat\":\n",
    "            self.squat_init()\n",
    "        elif self.exercise == \"shoulder\":\n",
    "            self.shoulder_press_init()\n",
    "        elif self.exercise == \"knee\":\n",
    "            self.knee_raise_init()\n",
    "    \n",
    "    def squat_init(self):\n",
    "        self.up_angle = 160\n",
    "        self.down_angle = 130\n",
    "\n",
    "    def shoulder_press_init(self):\n",
    "        self.up_angle = 130\n",
    "        self.down_angle = 50\n",
    "        self.limit_angle = 110\n",
    "\n",
    "    def knee_raise_init(self):\n",
    "        self.up_angle = 70\n",
    "        self.down_angle = 160\n",
    "\n",
    "    def update(self, index, angle, passing):\n",
    "        if passing: return\n",
    "\n",
    "        if self.exercise == \"shoulder\":\n",
    "            if self.state[index] == \"up\" and angle > self.up_angle:\n",
    "                self.state[index] = \"down\"\n",
    "\n",
    "                if self.state[0] == \"down\" and self.state[1] == \"down\":\n",
    "                    self.count +=1\n",
    "            elif self.state[index] == \"down\":\n",
    "                if angle < self.down_angle:\n",
    "                    self.state[index] = \"up\"\n",
    "        elif self.exercise == \"squat\":\n",
    "            if self.state[index] == \"down\" and angle < self.down_angle:\n",
    "                self.state[index] = \"up\"\n",
    "\n",
    "                if self.state[0] == \"up\" and self.state[1] == \"up\":\n",
    "                    self.count += 1\n",
    "            elif self.state[index] == \"up\" and angle > self.up_angle:\n",
    "                self.state[index] = \"down\"\n",
    "        elif self.exercise == \"knee\":\n",
    "            if self.state[index] == \"up\" and angle < self.up_angle:\n",
    "                self.state[index] = \"down\"\n",
    "                self.count += 0.5\n",
    "            elif self.state[index] == \"down\" and angle > self.down_angle:\n",
    "                self.state[index] = \"up\"\n",
    "\n",
    "    def to_pixel(self, frame, idx, landmarks):\n",
    "        return np.array([\n",
    "            landmarks[idx].x * frame.shape[1],\n",
    "            landmarks[idx].y * frame.shape[0]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def calculate_angle(self, a, b, c):\n",
    "        a = np.array([a.x, a.y])\n",
    "        b = np.array([b.x, b.y])\n",
    "        c = np.array([c.x, c.y])\n",
    "        ba = a - b\n",
    "        bc = c - b\n",
    "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "        angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "        return np.degrees(angle)\n",
    "    \n",
    "    def get_point_by_angle(self, origin, center, from_point, angle_deg, length=None):\n",
    "        vec = from_point - center\n",
    "        if length is None:\n",
    "            length = np.linalg.norm(vec)\n",
    "\n",
    "        # 단위 벡터\n",
    "        unit_vec = vec / (np.linalg.norm(vec) + 1e-6)\n",
    "\n",
    "        # 회전 각도: 관절 기준이라 180도에서 빼야 함\n",
    "        theta = np.radians(180 - angle_deg)\n",
    "\n",
    "        rot_matrix = np.array([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta),  np.cos(theta)]\n",
    "        ])\n",
    "\n",
    "        rotated_vec = rot_matrix @ unit_vec\n",
    "        target_point = origin + rotated_vec * length\n",
    "        return target_point\n",
    "\n",
    "\n",
    "    def draw_exercise_line(self, frame, landmarks):\n",
    "        joint_map = {\n",
    "            \"shoulder\": [(12, 14, 16), (11, 13, 15)],\n",
    "            \"squat\": [(24, 26, 28), (23, 25, 27)],\n",
    "            \"knee\": [(24, 26, 28), (23, 25, 27)]\n",
    "        }\n",
    "\n",
    "        guide_angle = {\n",
    "            \"shoulder\": lambda idx: -120 * (1 if idx == 0 else -1),\n",
    "            \"squat\": lambda idx: 90 * (1 if idx == 1 else -1),\n",
    "            \"knee\": lambda idx: -98\n",
    "        }\n",
    "\n",
    "        for idx, (a, b, c) in enumerate(joint_map[self.exercise]):\n",
    "            passing = False\n",
    "            pt1 = self.to_pixel(frame, a, landmarks)\n",
    "            pt2 = self.to_pixel(frame, b, landmarks)\n",
    "            pt3 = self.to_pixel(frame, c, landmarks)\n",
    "\n",
    "            angle = self.calculate_angle(landmarks[a], landmarks[b], landmarks[c])\n",
    "            if (landmarks[c].y > landmarks[a].y and self.exercise == \"shoulder\"):\n",
    "                passing = True\n",
    "\n",
    "            self.update(idx, angle, passing)\n",
    "\n",
    "            if self.exercise == \"shoulder\":\n",
    "                origin = pt1\n",
    "                center = pt2\n",
    "                point = pt1\n",
    "            elif self.exercise == \"squat\":\n",
    "                origin = pt2\n",
    "                center = pt2\n",
    "                point = pt3\n",
    "            elif self.exercise == \"knee\":\n",
    "                origin = pt1\n",
    "                center = pt2\n",
    "                point = pt1\n",
    "\n",
    "            # 초기 가이드 벡터\n",
    "            if not self.initialized[idx]:\n",
    "                self.vectors[idx] = self.get_point_by_angle(origin, center, point, guide_angle[self.exercise](idx))\n",
    "                self.initialized[idx] = True\n",
    "\n",
    "            # 가이드 라인\n",
    "            cv2.line(frame, tuple(origin.astype(int)), tuple(self.vectors[idx].astype(int)), self.p, 2)\n",
    "\n",
    "            # 관절 색\n",
    "            if self.exercise == \"shoulder\":\n",
    "                if angle > self.up_angle and landmarks[c].y < landmarks[a].y:\n",
    "                    color = self.g \n",
    "                else:\n",
    "                    color = self.r\n",
    "            elif self.exercise == \"knee\":\n",
    "                color = self.g if angle < self.up_angle else self.r\n",
    "            elif self.exercise == \"squat\":\n",
    "                color = self.r if angle > self.down_angle else self.g\n",
    "\n",
    "            # 관절 라인\n",
    "            cv2.line(frame, tuple(pt1.astype(int)), tuple(pt2.astype(int)), color, 2)\n",
    "            cv2.line(frame, tuple(pt2.astype(int)), tuple(pt3.astype(int)), color, 2)\n",
    "\n",
    "            # 점 & 각도 표시\n",
    "            for pt in [pt1, pt2, pt3]:\n",
    "                cv2.circle(frame, tuple(pt.astype(int)), 4, self.b, -1)\n",
    "\n",
    "            cv2.putText(frame, str(int(angle)), (int(pt2[0]), int(pt2[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "\n",
    "class AngleGuidThreaded:\n",
    "    def __init__(self, exercise):\n",
    "        self.guid = AngleGuid(exercise)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def set_exercise(self, exercise):\n",
    "        with self.lock:\n",
    "            self.exercise = exercise\n",
    "\n",
    "    def draw(self, frame, landmarks):\n",
    "        with self.lock:\n",
    "            self.guid.draw_exercise_line(frame, landmarks)\n",
    "\n",
    "    def get_count(self):\n",
    "        with self.lock:\n",
    "            return self.guid.count\n",
    "    \n",
    "    def set_count(self):\n",
    "        with self.lock:\n",
    "            self.guid.count = 0\n",
    "\n",
    "    def get_current_angles(self):\n",
    "        with self.lock:\n",
    "            return self.guid.current_angle\n",
    "        \n",
    "    def get_state(self):\n",
    "        with self.lock:\n",
    "            return self.guid.state\n",
    "\n",
    "path = \"/home/shin/deeplearning-repo-1/dataset/stand_knee_raise/9.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "sequence = deque(maxlen=20)\n",
    "\n",
    "execrise_list = [\"Standing\", \"Standing Knee Raise\", \"Shoulder Press\", \"Squat\", \"Side Lunge\"]\n",
    "model = load_model('exercise_classifier.h5')\n",
    "\n",
    "lock = threading.Lock()\n",
    "predict_thread = PredictionThread(model, sequence, lock)\n",
    "predict_thread.start()\n",
    "\n",
    "last_exercise = None\n",
    "last_tts_time = 0  # TTS 중복 방지 타이머\n",
    "\n",
    "guid = AngleGuidThreaded(\"squat\")\n",
    "start_time = time.time()\n",
    "countdown_done = False\n",
    "\n",
    "prev_count = 0\n",
    "\n",
    "try:\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            if not countdown_done:\n",
    "                countdown = max(0, 5 - int(elapsed))\n",
    "                cv2.putText(image, f\"{countdown}...\", (image.shape[1] // 2, image.shape[0] // 2), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 5)\n",
    "                if countdown == 0:\n",
    "                    countdown_done = True\n",
    "            else:\n",
    "                #image = cv2.resize(image, (640, 480))\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = mp_pose.process(image)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                if results.pose_landmarks is None:\n",
    "                    continue\n",
    "\n",
    "                landmarks = extract_pose_landmarks(results)\n",
    "\n",
    "                with lock:\n",
    "                    sequence.append(landmarks)\n",
    "\n",
    "                guid.draw(image, results.pose_landmarks.landmark)\n",
    "                count = guid.get_count()\n",
    "                state = guid.get_state()\n",
    "\n",
    "                if predict_thread.result is not None:\n",
    "                    predict = predict_thread.result\n",
    "                    predict_class = int(np.argmax(predict))\n",
    "                    label = execrise_list[predict_class]\n",
    "\n",
    "                    # 이전에 예측된 운동과 현재 운동이 다를 경우 다시 예측된 운동명 TTS 재생\n",
    "                    if label != last_exercise:\n",
    "                        if time.time() - last_tts_time > 5:\n",
    "                            tts_thread = TextToSpeechThread(f\"{label}\")\n",
    "                            #tts_thread.start()\n",
    "                            last_exercise = label\n",
    "                            last_tts_time = time.time()\n",
    "\n",
    "                    # 화면에 예측 표시\n",
    "                    #cv2.putText(frame, f\"{label}\", (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "                    \n",
    "                cv2.putText(image, f\"Count: {int(count)}\", (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,255), 4)\n",
    "                cv2.putText(image, f\"left: {state[0]}, right: {state[1]}\", (10, 145), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,255), 4)\n",
    "\n",
    "                if count > prev_count:\n",
    "                    tts_thread = TextToSpeechThread(f\"{count}\")\n",
    "                    tts_thread.start()\n",
    "                    prev_count = count\n",
    "\n",
    "            cv2.imshow('Exercise Classifier', image)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "finally:\n",
    "    predict_thread.stop()\n",
    "    predict_thread.join()\n",
    "    cap.release()\n",
    "    mp_pose.close()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x, y좌표 + 관절 각도를 이용한 운동 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:41:18.463071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743057678.479619    4383 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743057678.485218    4383 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743057678.499100    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057678.499115    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057678.499117    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057678.499118    4383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 15:41:18.503961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743057680.927604    4383 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743057680.929935    6164 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743057681.023162    6150 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057681.060775    6153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743057681.079283    4383 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1176 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "W0000 00:00:1743057683.000224    6155 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "I0000 00:00:1743057685.415758    6193 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpbasly5fq.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpbasly5fq.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpitpypeuq.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpitpypeuq.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp2fu8wj3s.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmp2fu8wj3s.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpqyu0edv0.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpqyu0edv0.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp4i9ep5z4.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmp4i9ep5z4.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpnmt552aw.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpnmt552aw.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpthya3weh.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpthya3weh.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpb4p98am_.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpb4p98am_.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmpivxm4q20.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmpivxm4q20.mp3 finished.\n",
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.32.5; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: /tmp/\n",
      "Playing MPEG stream 1 of 1: tmp8l8a7edw.mp3 ...\n",
      "\n",
      "MPEG 2.0 L III cbr64 24000 mono\n",
      "\n",
      "[0:01] Decoding of tmp8l8a7edw.mp3 finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import mediapipe as mp\n",
    "from collections import deque, Counter\n",
    "import threading\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "from gtts import gTTS\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def extract_pose_angles(results, frame):\n",
    "    lm = results.pose_landmarks.landmark\n",
    "    image_height, image_width, _ = frame.shape\n",
    "\n",
    "    neck_x = (lm[11].x + lm[12].x) / 2\n",
    "    neck_y = (lm[11].y + lm[12].y) / 2\n",
    "    landmarks = [(lm[i].x - neck_x, lm[i].y - neck_y) for i in range(33)]\n",
    "\n",
    "    angle_joints = [\n",
    "        (11, 13, 15), (12, 14, 16),  # 양팔\n",
    "        (23, 25, 27), (24, 26, 28),  # 양다리\n",
    "        (13, 11, 23), (14, 12, 24),  # 상체\n",
    "        (11, 23, 25), (12, 24, 26),  # 골반\n",
    "    ]\n",
    "\n",
    "    landmark_points = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "    xy_features = []\n",
    "    for i in landmark_points:\n",
    "        x, y = landmarks[i]\n",
    "        xy_features.extend([x, y])\n",
    "\n",
    "    angles = []\n",
    "    features_per_video = []\n",
    "    for a, b, c in angle_joints:\n",
    "        angle = calculate_angle(landmarks[a], landmarks[b], landmarks[c])\n",
    "        angles.append(angle)\n",
    "\n",
    "        cx = int(lm[b].x * image_width)\n",
    "        cy = int(lm[b].y * image_height)\n",
    "        cv2.putText(frame, f\"{int(angle)}\", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    full_feature = xy_features + angles\n",
    "    features_per_video.append(full_feature)\n",
    "    \n",
    "    return features_per_video\n",
    "\n",
    "class TextToSpeechThread(threading.Thread):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            tts = gTTS(text=self.text, lang='ko')\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmpfile:\n",
    "                tts.save(tmpfile.name)\n",
    "                os.system(f\"mpg123 {tmpfile.name}\")\n",
    "                os.remove(tmpfile.name)\n",
    "        except Exception as e:\n",
    "            print(f\"[TTS Error] {e}\")\n",
    "\n",
    "class PredictionThread(threading.Thread):\n",
    "    def __init__(self, model, sequence, lock):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sequence = sequence\n",
    "        self.lock = lock\n",
    "        self.result = None\n",
    "        self.running = True\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            with self.lock:\n",
    "                if len(self.sequence) == 20:\n",
    "                    input_data = np.array(list(self.sequence)).reshape(1, 20, 32)\n",
    "                else:\n",
    "                    input_data = None\n",
    "\n",
    "            if input_data is not None:\n",
    "                prediction = self.model.predict(input_data, verbose=0)\n",
    "                self.result = prediction\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "sequence = deque(maxlen=20)\n",
    "\n",
    "execrise_list = [\"Standing\", \"Standing Knee Raise\", \"Shoulder Press\", \"Squat\", \"Front Lunge\"]\n",
    "model = load_model(\"exercise_classifier_angle.h5\")\n",
    "\n",
    "lock = threading.Lock()\n",
    "predict_thread = PredictionThread(model, sequence, lock)\n",
    "predict_thread.start()\n",
    "\n",
    "fix_exercise = False\n",
    "fixed_class = None\n",
    "last_tts_time = 0\n",
    "last_exercise = None\n",
    "\n",
    "guid = AngleGuid()\n",
    "guid.init(\"shoulder\")\n",
    "guid.set_direction(\"up\")\n",
    "\n",
    "try:\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # frame = cv2.resize(frame, (1280, 720))\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = mp_pose.process(image)\n",
    "            if results.pose_landmarks is None:\n",
    "                continue\n",
    "\n",
    "            features = extract_pose_angles(results, image)\n",
    "            with lock:\n",
    "                sequence.append(features)\n",
    "\n",
    "            if predict_thread.result is not None:\n",
    "                predict = predict_thread.result\n",
    "                predict_class = int(np.argmax(predict))\n",
    "                label = execrise_list[predict_class]\n",
    "\n",
    "                # 이전에 예측된 운동과 현재 운동이 다를 경우 다시 예측된 운동명 TTS 재생\n",
    "                if label != last_exercise:\n",
    "                    if time.time() - last_tts_time > 5:\n",
    "                        tts_thread = TextToSpeechThread(f\"{label}\")\n",
    "                        tts_thread.start()\n",
    "                        last_tts_time = time.time()\n",
    "                        last_exercise = label\n",
    "\n",
    "                cv2.putText(frame, f\"{label}\", (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "\n",
    "            cv2.imshow('Exercise Classifier', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "finally:\n",
    "    predict_thread.stop()\n",
    "    predict_thread.join()\n",
    "    cap.release()\n",
    "    mp_pose.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
