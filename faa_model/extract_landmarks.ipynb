{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:46:07.707769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743057967.724775   52760 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743057967.730345   52760 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743057967.742767   52760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057967.742795   52760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057967.742796   52760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743057967.742797   52760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 15:46:07.746800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743057970.532649   52760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743057970.535507   52863 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1743057970.545531   52760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743057970.548442   52878 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]W0000 00:00:1743057970.605185   52858 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057970.609213   52865 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057970.649182   52848 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057970.655908   52875 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057970.673850   52868 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "100%|██████████| 19/19 [02:58<00:00,  9.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 경로\n",
    "DATASET_DIR = '/home/shin/deeplearning-repo-1/dataset/front_lunge'\n",
    "\n",
    "# 좌표 추출 운동 설정\n",
    "exericse_name = \"프론트 런지\"\n",
    "\n",
    "pickle_name = exericse_name + \".pkl\"\n",
    "\n",
    "exericse_type = [\"가만히\", \"스탠딩 니업\", \"숄더프레스\", \"스쿼트\", \"프론트 런지\", \"푸시업\", \"브릿지\", \"스탠딩 바이시클\", \"사이드 런지\"]\n",
    "exericse_label = exericse_type.index(exericse_name)\n",
    "\n",
    "# 시퀀스 사이즈\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# mediapipe 선언\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "landmark_points = [ 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28 ]\n",
    "\n",
    "with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5) as pose:\n",
    "    for video_file in tqdm(os.listdir(DATASET_DIR)):\n",
    "        video_path = os.path.join(DATASET_DIR, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = []\n",
    "\n",
    "                lm = results.pose_landmarks.landmark\n",
    "\n",
    "                # 기준점\n",
    "                neck_x = (lm[11].x + lm[12].x) / 2\n",
    "                neck_y = (lm[11].y + lm[12].y) / 2\n",
    "                #neck_z = (lm[11].z + lm[12].z) / 2\n",
    "\n",
    "                for idx in landmark_points:\n",
    "                    x = lm[idx].x - neck_x\n",
    "                    y = lm[idx].y - neck_y\n",
    "                    #z = lm[idx].z - neck_z\n",
    "                    landmarks.extend([x, y])\n",
    "                frames.append(landmarks)\n",
    "                if len(frames) >= SEQUENCE_LENGTH:\n",
    "                    sequences.append(frames[-SEQUENCE_LENGTH:])\n",
    "                    labels.append(exericse_label)\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프론트 런지 데이터 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "with open(pickle_name, 'wb') as f:\n",
    "    pickle.dump((sequences, labels), f)\n",
    "\n",
    "    print(exericse_name + \" 데이터 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743057130.271184  461759 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743057130.284971  928604 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]W0000 00:00:1743057130.381059  928591 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743057130.460806  928596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|██████████| 20/20 [03:12<00:00,  9.61s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# === 설정값\n",
    "DATASET_DIR = '/home/shin/deeplearning-repo-1/dataset/front_lunge'\n",
    "exericse_name = \"프론트 런지\"\n",
    "pickle_name = exericse_name + \".pkl\"\n",
    "exericse_type = [\"가만히\", \"스탠딩 니업\", \"숄더프레스\", \"스쿼트\", \"프론트 런지\", \"푸시업\", \"브릿지\", \"스탠딩 바이시클\", \"사이드 런지\"]\n",
    "exericse_label = exericse_type.index(exericse_name)\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# === 각도 계산 함수\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# === MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# === 결과 저장 리스트\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# === 관절 선택\n",
    "landmark_points = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "# === 각도 조합 정의\n",
    "angle_joints = [\n",
    "    (11, 13, 15), (12, 14, 16),  # 양팔\n",
    "    (23, 25, 27), (24, 26, 28),  # 양다리\n",
    "    (13, 11, 23), (14, 12, 24),  # 상체\n",
    "    (11, 23, 25), (12, 24, 26),  # 골반\n",
    "]\n",
    "\n",
    "# === 데이터 수집\n",
    "for video_file in tqdm(os.listdir(DATASET_DIR)):\n",
    "    video_path = os.path.join(DATASET_DIR, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features_per_video = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "        if not results.pose_landmarks:\n",
    "            continue\n",
    "\n",
    "        lm = results.pose_landmarks.landmark\n",
    "        neck_x = (lm[11].x + lm[12].x) / 2\n",
    "        neck_y = (lm[11].y + lm[12].y) / 2\n",
    "        landmarks = [(lm[i].x - neck_x, lm[i].y - neck_y) for i in range(33)]\n",
    "\n",
    "        # x, y 좌표 (12개 관절만)\n",
    "        xy_features = []\n",
    "        for i in landmark_points:\n",
    "            x, y = landmarks[i]\n",
    "            xy_features.extend([x, y])\n",
    "\n",
    "        # 관절 각도 (8개)\n",
    "        angle_features = []\n",
    "        for a, b, c in angle_joints:\n",
    "            angle = calculate_angle(landmarks[a], landmarks[b], landmarks[c])\n",
    "            angle_features.append(angle)\n",
    "\n",
    "        full_feature = xy_features + angle_features\n",
    "        features_per_video.append(full_feature)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # === 시퀀스 단위로 분할\n",
    "    for i in range(len(features_per_video) - SEQUENCE_LENGTH):\n",
    "        seq = features_per_video[i:i + SEQUENCE_LENGTH]\n",
    "        sequences.append(seq)\n",
    "        labels.append(exericse_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프론트 런지 데이터 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# # === 결과 저장\n",
    "# output = {\n",
    "#     \"sequences\": sequences,\n",
    "#     \"labels\": labels\n",
    "# }\n",
    "\n",
    "with open(pickle_name, \"wb\") as f:\n",
    "    pickle.dump((sequences, labels), f)\n",
    "\n",
    "    print(exericse_name + \" 데이터 저장 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
